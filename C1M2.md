---
layout: Note
title: /C1M2/
permalink: /C1M2/
---

## ‚≠êÔ∏è M2 - The Data Engineering Ecosystem

#### ‚úîÔ∏è L1 - The Data Ecosystem and Languages for Data Professionals
- Video - Overview of the Data Engineering Ecosystem
- Video - Types of Data
- Video - Types of File Formats
- Video - Sources of Data
- Video - Languages for Data Professionals
- Video - Viewpoints: Working with Varied Data Sources and Types
- Read - Metadata and Metadata Management

#### ‚úîÔ∏è L2 - Data Repositories, Data Pipelines, and Data Integration Platforms

Video - Overview of Data Repositories
- Video - RDBMS
- Video - NoSQL
- Video - Data Warehouses, Data Marts, and Data Lakes
- Video - Viewpoints: Data Lakehouses Explained
- Video - Viewpoints: Considerations for Choice of Data Repository
- Video - ETL, ELT, and Data Pipelines
- Video - Data Integration Platforms
- Video - Viewpoints: Tools, Databases, and Data Repositories of Choice

#### ‚úîÔ∏è L3 - Big Data Platforms
- Video - Foundations of Big Data
- Video - Big Data Processing Tools: Hadoop, HDFS, Hive, and Spark
- Video - Viewpoints: Impact of Big Data on Data Engineering
- Lab - Create your IBM Cloud account
- Lab - Provision an instance of IBM Db2 Lite plan

## ‚≠êÔ∏è L1 - The Data Ecosystem and Languages for Data Professionals

### üóÇÔ∏è Video - Overview of the Data Engineering Ecosystem

#### ‚úîÔ∏è 01 - Data Engineer‚Äôs Ecosystem Overview
- **Data Engineer‚Äôs Ecosystem**: A collection of infrastructure, tools, frameworks, and processes supporting the full data lifecycle.
    - Covers extracting data from disparate sources.
    - Includes architecting and managing data pipelines.
        - Supports transformation, integration, storage, and workflow automation.
- **Workflow Scope**: End-to-end responsibility for data movement and usability.
    - From raw data ingestion to application development.
    - Ensures optimized data flow between systems.

#### ‚úîÔ∏è 02 - Data Classification by Structure
- **Structured Data**: Data with a rigid format organized into rows and columns.
    - Commonly found in databases and spreadsheets.
- **Semi-Structured Data**: Data with partial structure and inconsistent elements.
    - Example:
        - Emails
            - Structured parts: sender name, recipient name.
            - Unstructured parts: email body content.
- **Unstructured Data**: Complex, mostly qualitative data without a predefined schema.
    - Examples:
        - Photos
        - Videos
        - Text files
        - PDFs
        - Social media content

#### ‚úîÔ∏è 03 - Data Types, Sources, and Formats
- **Data Types Influence**: The structure of data determines storage and processing tools.
    - Impacts choice of data repositories.
    - Impacts tools used for querying and processing.
- **Data Sources**:
    - Relational databases
    - Non-relational databases
    - APIs
    - Web services
    - Data streams
    - Social platforms
    - Sensor devices
- **File Formats**: Data exists in many formats depending on source and use case.

#### ‚úîÔ∏è 04 - Data Repositories
- **Data Repositories**: Systems used to store and manage collected data.
    - Two main types:
        - Transactional
        - Analytical
- **Transactional Systems (OLTP)**: Designed for high-volume operational data.
    - Characteristics:
        - Supports day-to-day operations.
    - Examples:
        - Online banking transactions
        - ATM transactions
        - Airline bookings
    - Database Type:
        - Typically relational
        - Can also be non-relational
- **Analytical Systems (OLAP)**: Optimized for complex analytical queries.
    - Includes:
        - Relational databases
        - Non-relational databases
        - Data warehouses
        - Data marts
        - Data lakes
        - Big data stores
- **Repository Selection Factors**:
    - Type of data
    - Format of data
    - Source of data
    - Context of use

#### ‚úîÔ∏è 05 - Data Integration and Data Pipelines
- **Data Integration**: Process of combining data from disparate sources.
    - Produces a unified view of data.
    - Enables users to query and manipulate data via a single interface.
- **Data Integration Tools**:
    - Combine and harmonize data from multiple systems.
- **Data Pipelines**: Tools and processes managing data flow from source to destination.
    - Covers the entire data journey.
- **Integration Processes**:
    - Extract-Transform-and-Load (ETL)
    - Extract-Load-and-Transform (ELT)

#### ‚úîÔ∏è 06 - Languages in the Data Engineer‚Äôs Ecosystem
- **Language Categories**:
    - Query languages
    - Programming languages
    - Shell and scripting languages
- **Query Languages**:
    - Used for querying and manipulating data.
    - Example:
        - SQL
- **Programming Languages**:
    - Used for developing data applications.
    - Example:
        - Python
- **Shell and Scripting Languages**:
    - Used for repetitive operational tasks.
    - Support automation and system operations.

#### ‚úîÔ∏è 07 - BI and Reporting Tools
- **BI and Reporting Tools**: Tools for data visualization and reporting.
    - Collect data from multiple data sources.
    - Present data in visual formats.
- **Visual Outputs**:
    - Interactive dashboards
- **Data Access Modes**:
    - Real-time visualization
    - Scheduled updates
- **Usability**:
    - Drag-and-drop interfaces.
    - No programming knowledge required.
- **User Roles**:
    - Typically used by Data Analysts and BI Analysts.
    - Enabled and managed by Data Engineers.

#### ‚úîÔ∏è 08 - Automation and Ecosystem Characteristics
- **Automation Tools and Frameworks**: Support all stages of the data analytics process.
    - Enable efficiency and scalability.
- **Ecosystem Characteristics**:
    - Diverse
    - Rich
    - Challenging
- **Learning Path**:
    - Different components of the ecosystem explored in greater detail later in the course.

### üóÇÔ∏è Video - Types of Data

#### ‚úîÔ∏è 01 - Definition and Nature of Data
- **Data**: Unorganized information that is processed to make it meaningful.
    - Comprises facts, observations, perceptions, numbers, characters, symbols, and images.
    - Can be interpreted to derive meaning.
- **Data Categorization**: One way to classify data is by its structure.
    - Structured
    - Semi-structured
    - Unstructured

#### ‚úîÔ∏è 02 - Structured Data
- **Structured Data**: Data with a well-defined structure that adheres to a specified data model.
    - Stored in well-defined schemas such as databases.
    - Often represented in tabular form with rows and columns.
- **Characteristics of Structured Data**:
    - Objective facts and numbers.
    - Can be collected, exported, stored, and organized easily.
- **Sources of Structured Data**:
    - SQL Databases
    - Online Transaction Processing (OLTP) Systems
        - Focus on business transactions.
    - Spreadsheets
        - Excel
        - Google Spreadsheets
    - Online forms
    - Sensors
        - Global Positioning Systems (GPS)
        - Radio Frequency Identification (RFID) tags
    - Network and Web server logs
- **Storage and Analysis**:
    - Typically stored in relational or SQL databases.
    - Easily examined using standard data analysis methods and tools.

#### ‚úîÔ∏è 03 - Semi-Structured Data
- **Semi-Structured Data**: Data that has some organizational properties but lacks a rigid schema.
    - Cannot be stored strictly as rows and columns.
- **Organizational Features**:
    - Uses tags, elements, or metadata.
    - Organized hierarchically.
- **Sources of Semi-Structured Data**:
    - E-mails
    - XML and other markup languages
    - Binary executables
    - TCP/IP packets
    - Zipped files
    - Integration of data from different sources
- **Common Formats**:
    - XML
        - Allows user-defined tags and attributes.
    - JSON
        - Stores data in a hierarchical structure.
    - Widely used for storing and exchanging semi-structured data.

#### ‚úîÔ∏è 04 - Unstructured Data
- **Unstructured Data**: Data without an easily identifiable structure.
    - Cannot be organized into rows and columns.
    - Does not follow a specific format, sequence, semantics, or rules.
- **Characteristics**:
    - Handles heterogeneous data sources.
    - Used in various business intelligence and analytics applications.
- **Sources of Unstructured Data**:
    - Web pages
    - Social media feeds
    - Images
        - JPEG
        - GIF
        - PNG
    - Video and audio files
    - Documents and PDF files
    - PowerPoint presentations
    - Media logs
    - Surveys
- **Storage Options**:
    - Files and documents for manual analysis.
    - NoSQL databases with specialized analysis tools.

#### ‚úîÔ∏è 05 - Structural Comparison Summary
- **Structured Data**: Well organized, database-ready, and suitable for standard analysis tools.
- **Semi-Structured Data**: Partially organized, relies on metadata for grouping and hierarchy.
- **Unstructured Data**: Not conventionally organized, lacks predefined schemas.

### üóÇÔ∏è Video - Types of File Formats

#### ‚úîÔ∏è 01 - File Types and Format Awareness
- **Data File Types and Formats**: A data professional works with many kinds of data files.
    - Requires understanding underlying file structure.
    - Helps evaluate benefits and limitations.
- **Format Selection Purpose**: Choosing the right format supports data and performance needs.
    - Informed decisions improve efficiency and compatibility.

#### ‚úîÔ∏è 02 - Overview of Common File Formats
- **Standard File Formats Covered**:
    - Delimited text file formats
    - Microsoft Excel Open XML Spreadsheet (XLSX)
    - Extensible Markup Language (XML)
    - Portable Document Format (PDF)
    - JavaScript Object Notation (JSON)

#### ‚úîÔ∏è 03 - Delimited Text File Formats
- **Delimited Text Files**: Text files where values in each row are separated by a delimiter.
    - Each line represents a row or record.
- **Delimiter**:
    - A character or sequence of characters marking value boundaries.
    - Common delimiters:
        - Comma
        - Tab
        - Colon
        - Vertical bar
        - Space
- **Common Delimited File Types**:
    - Comma-Separated Values (CSV)
        - Uses a comma as delimiter.
    - Tab-Separated Values (TSV)
        - Uses a tab as delimiter.
- **CSV vs TSV Usage**:
    - TSV is preferred when literal commas appear in text data.
        - Tab stops are infrequent in running text.
- **File Structure**:
    - First row acts as a column header.
    - Each column may have a different data type.
        - Examples:
            - Date
            - String
            - Integer
- **Advantages**:
    - Allows field values of any length.
    - Considered a standard format for simple schemas.
    - Can be processed by almost all applications.
- **Delimiter Role in Data Streams**:
    - One of several methods to specify boundaries in data streams.

#### ‚úîÔ∏è 04 - Microsoft Excel Open XML Spreadsheet (XLSX)
- **XLSX**: A spreadsheet file format based on XML.
    - Created by Microsoft.
- **Workbook Structure**:
    - A workbook can contain multiple worksheets.
    - Each worksheet consists of:
        - Rows
        - Columns
        - Cells
            - Each cell contains data.
- **File Characteristics**:
    - Uses an open file format.
        - Accessible to many applications.
    - Supports all Excel functions.
- **Security Aspect**:
    - Cannot store malicious code.
    - Considered one of the more secure file formats.

#### ‚úîÔ∏è 05 - Extensible Markup Language (XML)
- **XML**: A markup language with rules for encoding data.
    - Readable by both humans and machines.
- **Design Purpose**:
    - Self-descriptive language.
    - Designed for sending information over the internet.
- **XML vs HTML**:
    - Similar in structure.
    - XML does not use predefined tags.
- **Interoperability**:
    - Platform independent.
    - Programming language independent.
    - Simplifies data sharing across systems.

#### ‚úîÔ∏è 06 - Portable Document Format (PDF)
- **PDF**: A document format developed by Adobe.
    - Presents documents independently of software, hardware, and operating systems.
- **Key Feature**:
    - Displays consistently across devices.
- **Common Use Cases**:
    - Legal documents
    - Financial documents
    - Fillable forms

#### ‚úîÔ∏è 07 - JavaScript Object Notation (JSON)
- **JSON**: A text-based open standard for transmitting structured data over the web.
    - Language-independent data format.
- **Compatibility**:
    - Can be read by any programming language.
    - Works across a wide range of browsers.
- **Advantages**:
    - Easy to use.
    - Suitable for sharing data of any size and type.
        - Includes audio and video.
- **Common Usage**:
    - Frequently used by APIs and Web Services to return data.

### üóÇÔ∏è Video - Sources of Data

#### ‚úîÔ∏è 01 - Diversity of Modern Data Sources
- **Data Sources**: Have become highly dynamic and diverse.
    - Include both internal and external sources.
- **Common Data Source Categories**:
    - Relational Databases
    - Flat Files and XML Datasets
    - APIs and Web Services
    - Web Scraping
    - Data Streams and Feeds

#### ‚úîÔ∏è 02 - Relational Databases as Data Sources
- **Internal Applications**: Support daily business operations.
    - Business activities
    - Customer transactions
    - Human resource activities
    - Workflows
- **Relational Databases**:
    - SQL Server
    - Oracle
    - MySQL
    - IBM DB2
- **Stored Data Characteristics**:
    - Structured data.
    - Used by databases and data warehouses.
- **Analytical Use Cases**:
    - Retail transaction data
        - Analyze sales across regions.
    - Customer relationship management data
        - Support sales projections.

#### ‚úîÔ∏è 03 - External Datasets
- **Public Datasets**:
    - Released by government organizations.
    - Include demographic and economic data.
- **Commercial Data Providers**:
    - Sell specialized datasets.
        - Point-of-Sale data
        - Financial data
        - Weather data
- **Business Uses**:
    - Strategy definition
    - Demand prediction
    - Distribution decisions
    - Marketing promotions
- **Common Delivery Formats**:
    - Flat files
    - Spreadsheet files
    - XML documents

#### ‚úîÔ∏è 04 - Flat Files
- **Flat Files**: Store data in plain text format.
    - One record or row per line.
    - Values separated by delimiters.
- **Delimiters**:
    - Commas
    - Semi-colons
    - Tabs
- **Data Structure**:
    - Maps to a single table.
    - Differs from relational databases with multiple tables.
- **Common Flat-File Format**:
    - CSV
        - Values separated by commas.

#### ‚úîÔ∏è 05 - Spreadsheet Files
- **Spreadsheet Files**: Specialized flat files with tabular structure.
    - Organized into rows and columns.
- **Worksheet Structure**:
    - A spreadsheet can contain multiple worksheets.
    - Each worksheet maps to a different table.
- **File Characteristics**:
    - Data stored as plain text.
    - Can include:
        - Formatting
        - Formulas
        - Additional metadata
- **Popular Spreadsheet Tools**:
    - Microsoft Excel
        - .XLS
        - .XLSX
    - Google Sheets
    - Apple Numbers
    - LibreOffice

#### ‚úîÔ∏è 06 - XML Datasets
- **XML Files**: Store data using tags.
    - Data values are marked up with tags.
- **Structural Capability**:
    - Supports complex data structures.
        - Hierarchical data
- **Comparison with Flat Files**:
    - Flat files map to a single table.
    - XML supports nested and hierarchical relationships.
- **Common XML Use Cases**:
    - Online survey data
    - Bank statements
    - Other unstructured datasets

#### ‚úîÔ∏è 07 - APIs and Web Services
- **APIs and Web Services**: Interfaces for accessing data programmatically.
    - Used by users and applications.
- **Interaction Model**:
    - Listen for incoming requests.
        - Web requests
        - Network requests
- **Response Formats**:
    - Plain text
    - XML
    - HTML
    - JSON
    - Media files
- **Popular API Use Cases**:
    - Social Media APIs
        - Twitter
        - Facebook
            - Opinion mining
            - Sentiment analysis
                - Measure appreciation and criticism.
    - Stock Market APIs
        - Share prices
        - Commodity prices
        - Earnings per share
        - Historical prices
    - Data Lookup and Validation APIs
        - Data cleaning
        - Data preparation
        - Data correlation
            - Example:
                - Mapping postal or zip codes to cities or states
- **Internal and External Access**:
    - APIs used to pull data from databases within and outside organizations.

#### ‚úîÔ∏è 08 - Web Scraping
- **Web Scraping**: Technique for extracting data from unstructured web sources.
    - Also known as:
        - Screen scraping
        - Web harvesting
        - Web data extraction
- **Functionality**:
    - Downloads specific data based on defined parameters.
- **Extractable Data Types**:
    - Text
    - Contact information
    - Images
    - Videos
    - Product items
- **Common Use Cases**:
    - Product price comparison
    - Sales lead generation
    - Forum and community data extraction
    - Machine learning training and testing datasets
- **Popular Web Scraping Tools**:
    - BeautifulSoup
    - Scrapy
    - Pandas
    - Selenium

#### ‚úîÔ∏è 09 - Data Streams
- **Data Streams**: Continuous flows of data from multiple sources.
    - Instruments
    - IoT devices
    - Applications
    - GPS systems
    - Websites
    - Social media platforms
- **Data Characteristics**:
    - Timestamped
    - Geo-tagged
- **Stream Use Cases**:
    - Stock and market tickers
        - Financial trading
    - Retail transaction streams
        - Demand prediction
        - Supply chain management
    - Surveillance and video feeds
        - Threat detection
    - Social media feeds
        - Sentiment analysis
    - Sensor data feeds
        - Monitoring industrial or farming machinery
    - Web click feeds
        - Web performance monitoring
        - Design improvement
    - Real-time flight events
        - Rebooking
        - Rescheduling
- **Stream Processing Applications**:
    - Apache Kafka
    - Apache Spark Streaming
    - Apache Storm

#### ‚úîÔ∏è 10 - RSS Feeds
- **RSS Feeds**: Really Simple Syndication feeds.
    - Used for capturing frequently updated data.
- **Common Sources**:
    - Online forums
    - News sites
- **Update Mechanism**:
    - Data refreshed continuously.
- **Feed Readers**:
    - Convert RSS text files into data streams.
    - Stream updates directly to user devices.

### üóÇÔ∏è Video - Languages for Data Professionals

#### ‚úîÔ∏è 01 - Language Categories for Data Professionals
- **Languages for Data Professionals**: Core tools used in data-related work.
    - Categorized into query languages, programming languages, and shell scripting.
- **Proficiency Requirement**: At least one language in each category is essential.
    - Supports data access, application development, and operational automation.
- **Language Categories**:
    - Query languages
    - Programming languages
    - Shell and scripting languages

#### ‚úîÔ∏è 02 - Query Languages
- **Query Languages**: Designed for accessing and manipulating data in databases.
- **SQL (Structured Query Language)**: A querying language for relational databases.
    - Used mostly, though not exclusively, with relational databases.
- **SQL Operations**:
    - Insert, update, and delete records.
    - Create databases, tables, and views.
    - Write stored procedures
        - A reusable set of instructions.
- **Advantages of SQL**:
    - Platform independent and portable.
    - Works with many databases and data repositories.
        - Vendors may add variations and extensions.
    - Simple, English-like syntax.
        - Uses keywords such as select, insert, into, update.
    - Requires fewer lines of code.
    - Retrieves large volumes of data efficiently.
    - Interpreter-based execution.
        - Enables fast prototyping.
    - Large user community and extensive documentation.
    - Provides a uniform platform worldwide.

#### ‚úîÔ∏è 03 - Programming Languages Overview
- **Programming Languages**: Designed for developing applications and controlling behavior.
    - Used for computation, analytics, and application logic.
- **Common Programming Languages**:
    - Python
    - R
    - Java

#### ‚úîÔ∏è 04 - Python
- **Python**: An open-source, general-purpose, high-level programming language.
    - Widely used across industries.
- **Language Characteristics**:
    - Expressive syntax with fewer lines of code.
    - Easy to learn with a low learning curve.
    - Emphasis on readability and simplicity.
- **Use in Data Work**:
    - Suitable for high-computational tasks on large datasets.
    - Efficient handling of time-consuming computations.
- **Libraries for Data Processing**:
    - NumPy and Pandas
        - Enable parallel processing.
- **Programming Paradigms Supported**:
    - Object-oriented
    - Imperative
    - Functional
    - Procedural
- **Reasons for Popularity**:
    - Easy to learn and use.
    - Open-source and free.
    - Runs on Windows and Linux.
    - Portable across platforms.
    - Strong community support.
- **Python Libraries**:
    - Pandas
        - Data cleaning and analysis.
    - NumPy and SciPy
        - Statistical analysis.
    - BeautifulSoup and Scrapy
        - Web scraping.
    - Matplotlib and Seaborn
        - Data visualization
            - Bar graphs
            - Histograms
            - Pie charts
    - OpenCV
        - Image processing.

#### ‚úîÔ∏è 05 - R
- **R**: An open-source programming language and environment.
    - Focused on data analysis, visualization, machine learning, and statistics.
- **Primary Strength**:
    - Creation of compelling visualizations.
- **Key Benefits of R**:
    - Platform independent and open-source.
    - Can be paired with other languages.
        - Example:
            - Python
    - Highly extensible.
        - Developers can define new functions.
    - Handles both structured and unstructured data.
    - Comprehensive data capabilities.
- **R Libraries**:
    - ggplot2
    - Plotly
        - Provide aesthetic graphical plots.
- **Advanced Capabilities**:
    - Report generation with embedded data and scripts.
    - Interactive web applications.
    - Dominant language for statistical tool development.

#### ‚úîÔ∏è 06 - Java
- **Java**: An object-oriented, class-based, platform-independent programming language.
    - Originally developed by Sun Microsystems.
- **Industry Position**:
    - Among the top-ranked programming languages.
- **Use in Data Analytics**:
    - Data cleaning.
    - Data import and export.
    - Statistical analysis.
    - Data visualization.
- **Big Data Ecosystem**:
    - Many big data frameworks are written in Java.
        - Hadoop
        - Hive
        - Spark
- **Performance**:
    - Well suited for speed-critical projects.

#### ‚úîÔ∏è 07 - Shell and Scripting Languages
- **Shell and Scripting Languages**: Used for repetitive and operational tasks.
- **Unix/Linux Shell**: A scripting environment for UNIX systems.
    - Scripts are plain text files with UNIX commands.
- **Shell Script Characteristics**:
    - Fast and easy to write.
    - Ideal for repetitive tasks.
- **Typical Shell Script Operations**:
    - File manipulation.
    - Program execution.
    - System administration tasks.
        - Disk backups.
        - System log evaluation.
    - Installation scripts.
    - Routine backups.
    - Batch executions.

#### ‚úîÔ∏è 08 - PowerShell
- **PowerShell**: A cross-platform automation and configuration framework by Microsoft.
    - Includes a command-line shell and scripting language.
- **Optimization Focus**:
    - Works well with structured data formats.
- **Supported Data Formats**:
    - JSON
    - CSV
    - XML
    - REST APIs
- **Object-Based Design**:
    - Enables operations such as:
        - Filtering
        - Sorting
        - Grouping
        - Comparing
- **Additional Use Cases**:
    - Data mining.
    - GUI development.
    - Creating charts, dashboards, and interactive reports.

### üóÇÔ∏è Video - Viewpoints: Working with Varied Data Sources and Types

### üóÇÔ∏è Read - Metadata and Metadata Management

## ‚≠êÔ∏è L2 - Data Repositories, Data Pipelines, and Data Integration Platforms

### üóÇÔ∏è Video - Overview of Data Repositories

#### ‚úîÔ∏è 01 - Data Repository Overview
- **Data Repository**: A general term for data that is collected, organized, and isolated.
    - Used for business operations, reporting, and data analysis.
- **Repository Scope**:
    - Can be small or large database infrastructure.
    - May include one or more databases that collect, manage, and store datasets.
- **Repository Types Introduced**:
    - Databases
    - Data Warehouses
    - Big Data Stores

#### ‚úîÔ∏è 02 - Databases and DBMS
- **Database**: A collection of data designed for input, storage, search, retrieval, and modification.
- **Database Management System (DBMS)**: A set of programs that creates and maintains databases.
    - Enables storing, modifying, and extracting data.
    - Uses querying to access information.
- **Querying Example**:
    - Retrieve customers inactive for six months or more.
- **Terminology Note**:
    - Database and DBMS are different concepts.
    - Often used interchangeably in practice.

#### ‚úîÔ∏è 03 - Database Selection Factors
- **Database Choice Factors**:
    - Data type and structure
    - Querying mechanisms
    - Latency requirements
    - Transaction speeds
    - Intended use of data

#### ‚úîÔ∏è 04 - Relational Databases
- **Relational Databases (RDBMS)**: Databases organized in tables with rows and columns.
    - Follow a well-defined structure and schema.
- **Design Basis**:
    - Built on flat file organizational principles.
- **Key Advantage**:
    - Optimized for operations and queries across many tables.
    - Handles large data volumes efficiently.
- **SQL**: Standard querying language for relational databases.

#### ‚úîÔ∏è 05 - Non-Relational Databases (NoSQL)
- **Non-Relational Databases (NoSQL)**: Databases that do not rely on fixed schemas.
    - Also known as ‚ÄúNot Only SQL‚Äù.
- **Motivation for NoSQL**:
    - Rapid growth in data volume, diversity, and speed.
    - Driven by cloud computing, IoT, and social media.
- **Core Characteristics**:
    - Built for speed, flexibility, and scale.
    - Support schema-less or free-form data storage.
- **Primary Usage**:
    - Widely used for big data processing.

#### ‚úîÔ∏è 06 - Data Warehouses
- **Data Warehouse**: A central repository for consolidated data.
    - Integrates data from disparate sources.
- **ETL Process**:
    - Extract
        - Collect data from multiple sources.
    - Transform
        - Clean and prepare data for use.
    - Load
        - Store data into the enterprise repository.
- **Purpose**:
    - Enable analytics and business intelligence.

#### ‚úîÔ∏è 07 - Data Marts and Data Lakes
- **Related Concepts**:
    - Data Marts
    - Data Lakes
- **Historical Architecture**:
    - Traditionally built on relational databases.
- **Modern Evolution**:
    - Adoption of NoSQL technologies.
    - Support for new and diverse data sources.
    - Use of non-relational repositories for data warehousing.

#### ‚úîÔ∏è 08 - Big Data Stores
- **Big Data Stores**: Repositories designed for very large datasets.
    - Include distributed storage and computational infrastructure.
- **Capabilities**:
    - Store massive volumes of data.
    - Scale horizontally.
    - Process data efficiently at large scale.

#### ‚úîÔ∏è 09 - Role of Data Repositories
- **Business Value**:
    - Isolate data for efficient access.
    - Improve credibility of reporting and analytics.
- **Additional Function**:
    - Serve as long-term data archives.

### üóÇÔ∏è Video - RDBMS

#### ‚úîÔ∏è 01 - Relational Database Fundamentals
- **Relational Database**: A collection of data organized into tables that can be linked by common data.
    - Tables are related through shared fields.
- **Table Structure**:
    - Rows
        - Represent records.
    - Columns
        - Represent attributes.
- **Customer Table Example**:
    - Attributes:
        - Company ID
        - Company Name
        - Company Address
        - Company Primary Phone
    - Each row represents one customer record.

#### ‚úîÔ∏è 02 - Table Relationships and Querying
- **Related Tables**: Tables linked using common fields.
    - Example:
        - Customer Table
        - Transaction Table
- **Transaction Table Attributes**:
    - Transaction Date
    - Customer ID
    - Transaction Amount
    - Payment Method
- **Relationship Key**:
    - Customer ID
        - Common field connecting customers and transactions.
- **Query Capability**:
    - Retrieve new result tables from one or more tables.
    - Enables consolidated reports.
        - Example:
            - Customer statements for a given time period.
- **Business Value**:
    - Understand relationships among data.
    - Generate new insights for decision-making.

#### ‚úîÔ∏è 03 - SQL and Data Organization
- **Structured Query Language (SQL)**: Standard language for querying relational databases.
    - Used for data retrieval and manipulation.
- **Design Foundation**:
    - Based on flat file principles.
        - Rows and columns.
        - Well-defined schema.
- **Key Difference from Spreadsheets**:
    - Optimized for large data volumes.
    - Supports complex queries across multiple tables.

#### ‚úîÔ∏è 04 - Core Characteristics of Relational Databases
- **Optimized Data Handling**:
    - Efficient storage, retrieval, and processing.
- **Schema and Structure**:
    - Each table has a unique structure.
    - Relationships reduce data redundancy.
- **Data Integrity**:
    - Fields restricted to specific data types and values.
    - Minimizes irregularities.
    - Ensures consistency.
- **Performance**:
    - SQL enables processing millions of records quickly.
- **Security Architecture**:
    - Controlled data access.
    - Enforcement of governance standards and policies.

#### ‚úîÔ∏è 05 - Deployment Models and Database Types
- **System Scale**:
    - Small desktop systems.
    - Large cloud-based systems.
- **Licensing Models**:
    - Open-source (internally supported).
    - Open-source with commercial support.
    - Commercial closed-source systems.
- **Popular Relational Databases**:
    - IBM DB2
    - Microsoft SQL Server
    - MySQL
    - Oracle Database
    - PostgreSQL

#### ‚úîÔ∏è 06 - Cloud-Based Relational Databases
- **Database-as-a-Service (DBaaS)**: Cloud-hosted relational databases.
    - Leverage scalable compute and storage.
- **Popular Cloud Relational Databases**:
    - Amazon Relational Database Service (RDS)
    - Google Cloud SQL
    - IBM DB2 on Cloud
    - Oracle Cloud
    - SQL Azure
- **Technology Maturity**:
    - Well-documented.
    - Easy to learn.
    - Large pool of skilled professionals.

#### ‚úîÔ∏è 07 - Advantages of Relational Databases
- **Table Joins**: Ability to create meaningful information by joining tables.
- **Flexibility**:
    - Add or modify columns and tables.
    - Rename relations during runtime.
- **Reduced Redundancy**:
    - Data stored once and referenced via relationships.
        - Example:
            - Customer data stored only in customer table.
- **Backup and Disaster Recovery**:
    - Easy export and import.
    - Backup while database is running.
    - Cloud mirroring enables near-zero data loss.
- **ACID Compliance**:
    - Atomicity
    - Consistency
    - Isolation
    - Durability
    - Ensures reliable and accurate transactions.

#### ‚úîÔ∏è 08 - Use Cases of Relational Databases
- **Online Transaction Processing (OLTP)**:
    - High-volume transaction workloads.
    - Supports many users.
    - Fast insert, update, and delete operations.
    - Low latency and quick response times.
- **Data Warehousing (OLAP)**:
    - Optimized for analytical processing.
    - Historical data analysis.
    - Business intelligence use cases.
- **IoT Solutions**:
    - Lightweight data storage.
    - Fast ingestion from edge devices.
    - Speed-critical processing.

#### ‚úîÔ∏è 09 - Limitations of RDBMS
- **Data Type Limitations**:
    - Poor support for semi-structured and unstructured data.
- **Schema Constraints**:
    - Schema and data types must match for migration.
- **Field Length Limits**:
    - Data exceeding field size is not stored.
- **Analytics Constraints**:
    - Not ideal for large-scale unstructured analytics.

#### ‚úîÔ∏è 10 - Continued Relevance of RDBMS
- **Current Status**:
    - Still the predominant technology for structured data.
- **Context**:
    - Remains relevant despite big data, cloud computing, IoT, and social media growth.

### üóÇÔ∏è Video - NoSQL

#### ‚úîÔ∏è 01 - NoSQL Overview and Motivation
- **NoSQL**: A non-relational database design meaning ‚Äúnot only SQL‚Äù.
    - Emphasizes flexibility rather than rejecting SQL entirely.
- **Adoption Context**:
    - Gained popularity with cloud computing, big data, and high-volume web/mobile applications.
- **Design Goal**:
    - Optimized for scale, performance, and ease of use.
- **Schema Model**:
    - Uses flexible or schema-less data models.
        - Does not rely on fixed row/column/table structures.
- **Query Language**:
    - Typically does not use SQL.
        - Some systems support SQL or SQL-like interfaces.
- **Data Support**:
    - Can store structured, semi-structured, and unstructured data in any record.

#### ‚úîÔ∏è 02 - NoSQL Data Models
- **NoSQL Database Types**:
    - Key-value store
    - Document-based
    - Column-based
    - Graph-based

#### ‚úîÔ∏è 03 - Key-Value Stores
- **Key-Value Store**: Stores data as collections of key-value pairs.
    - Key is a unique identifier.
    - Values range from simple strings to complex JSON documents.
- **Typical Use Cases**:
    - User session data
    - User preferences
    - Real-time recommendations
    - Targeted advertising
    - In-memory caching
- **Limitations**:
    - Not ideal for querying by value.
    - Poor fit for relationships or multiple unique keys.
- **Examples**:
    - Redis
    - Memcached
    - DynamoDB

#### ‚úîÔ∏è 04 - Document-Based Databases
- **Document-Based Databases**: Store each record as a self-contained document.
    - Related data is kept together.
- **Key Capabilities**:
    - Flexible indexing
    - Ad hoc queries
    - Analytics over document collections
- **Typical Use Cases**:
    - eCommerce platforms
    - Medical records
    - CRM platforms
    - Analytics platforms
- **Limitations**:
    - Not suitable for complex search queries.
    - Limited support for multi-operation transactions.
- **Examples**:
    - MongoDB
    - Amazon DocumentDB
    - CouchDB
    - Cloudant

#### ‚úîÔ∏è 05 - Column-Based Databases
- **Column-Based Model**: Stores data by columns instead of rows.
    - Data is grouped into column families.
- **Column Family**:
    - Logical grouping of columns accessed together.
        - Example:
            - Customer profile data
            - Excludes purchase history
- **Performance Advantage**:
    - Columns stored contiguously on disk.
    - Enables fast reads and searches.
- **Typical Use Cases**:
    - Heavy write workloads
    - Time-series data
    - Weather data
    - IoT data
- **Limitations**:
    - Not suitable for complex queries.
    - Poor fit for frequently changing query patterns.
- **Examples**:
    - Cassandra
    - HBase

#### ‚úîÔ∏è 06 - Graph-Based Databases
- **Graph-Based Databases**: Use graph structures to store data.
    - Nodes represent data entities.
    - Edges represent relationships.
- **Strengths**:
    - Excellent for connected data.
    - Efficient relationship traversal.
- **Typical Use Cases**:
    - Social networks
    - Real-time product recommendations
    - Network diagrams
    - Fraud detection
    - Access management
- **Limitations**:
    - Not optimized for high-volume transactional analytics.
- **Examples**:
    - Neo4j
    - Cosmos DB

#### ‚úîÔ∏è 07 - Advantages of NoSQL
- **Data Flexibility**:
    - Handles structured, semi-structured, and unstructured data.
- **Distributed Architecture**:
    - Runs across multiple data centers.
    - Leverages cloud infrastructure.
- **Scalability**:
    - Cost-effective horizontal scale-out.
    - Adds capacity by adding nodes.
- **System Design Benefits**:
    - Simpler design.
    - Better availability control.
    - Improved agility and faster iteration.

#### ‚úîÔ∏è 08 - NoSQL vs Relational Databases
- **Schema Design**:
    - RDBMS:
        - Rigid schemas.
    - NoSQL:
        - Schema-agnostic.
- **Cost Model**:
    - RDBMS:
        - Expensive high-end commercial systems.
    - NoSQL:
        - Designed for low-cost commodity hardware.
- **Transaction Guarantees**:
    - RDBMS:
        - ACID-compliant.
    - NoSQL:
        - Often sacrifices ACID for scalability and performance.
- **Technology Maturity**:
    - RDBMS:
        - Mature and well-documented.
    - NoSQL:
        - Relatively newer with evolving standards.
- **Industry Trend**:
    - NoSQL databases are increasingly used in mission-critical applications.

### üóÇÔ∏è Video - Data Warehouses, Data Marts, and Data Lakes

#### ‚úîÔ∏è 01 - Data Mining Repositories Overview
- **Data Mining Repositories**: Systems designed to house data for reporting, analysis, and insights.
    - Share a common goal but differ in purpose, data types, and access patterns.
- **Repository Types Covered**:
    - Data Warehouses
    - Data Marts
    - Data Lakes

#### ‚úîÔ∏è 02 - Data Warehouse
- **Data Warehouse**: A central repository integrating data from multiple sources.
    - Serves as a single source of truth.
- **Stored Data Characteristics**:
    - Current and historical data.
    - Cleansed, conformed, and categorized.
    - Modeled and structured for specific analytical purposes.
        - Analysis-ready upon loading.
- **Traditional Data Sources**:
    - Transactional and operational systems.
        - CRM
        - ERP
        - HR
        - Finance
- **Evolving Architecture**:
    - Adoption of non-relational repositories.
    - Enabled by NoSQL technologies and new data sources.

#### ‚úîÔ∏è 03 - Data Warehouse Architecture
- **Three-Tier Architecture**:
    - Bottom Tier:
        - Database servers.
        - Relational, non-relational, or hybrid.
        - Extract data from multiple sources.
    - Middle Tier:
        - OLAP Server.
        - Processes and analyzes data across databases.
    - Top Tier:
        - Client front-end layer.
        - Tools for querying, reporting, and analysis.

#### ‚úîÔ∏è 04 - Cloud-Based Data Warehouses
- **Cloud Migration**: Shift from on-premise to cloud-based warehouses.
- **Cloud Benefits**:
    - Lower costs.
    - Virtually unlimited storage and compute.
    - Pay-as-you-go scalability.
    - Faster disaster recovery.
- **Use Case Fit**:
    - Large volumes of operational data.
    - Frequent reporting and analytics needs.
- **Popular Data Warehouse Platforms**:
    - Teradata Enterprise Data Warehouse
    - Oracle Exadata
    - IBM Db2 Warehouse on Cloud
    - IBM Netezza Performance Server
    - Amazon Redshift
    - Google BigQuery
    - Cloudera Enterprise Data Hub
    - Snowflake Cloud Data Warehouse

#### ‚úîÔ∏è 05 - Data Mart
- **Data Mart**: A subset of a data warehouse tailored to a specific business function.
    - Built for a particular purpose or user group.
- **Example User Groups**:
    - Sales teams
    - Finance teams
- **Types of Data Marts**:
    - Dependent Data Mart
    - Independent Data Mart
    - Hybrid Data Mart

#### ‚úîÔ∏è 06 - Types of Data Marts
- **Dependent Data Mart**:
    - Derived from an enterprise data warehouse.
    - Uses already cleaned and transformed data.
    - Provides:
        - Isolated security
        - Isolated performance
- **Independent Data Mart**:
    - Built directly from operational systems or external sources.
    - Requires full transformation of source data.
- **Hybrid Data Mart**:
    - Combines data from:
        - Data warehouses
        - Operational systems
        - External sources

#### ‚úîÔ∏è 07 - Purpose of Data Marts
- **Business Objectives**:
    - Provide relevant data to users when needed.
    - Accelerate business processes.
    - Enable cost- and time-efficient decision-making.
- **Performance and Governance**:
    - Improve end-user response time.
    - Provide secure access and control.

#### ‚úîÔ∏è 08 - Data Lake
- **Data Lake**: A repository for storing data in its native format.
    - Supports structured, semi-structured, and unstructured data.
- **Key Difference from Data Warehouse**:
    - No predefined schema required before loading.
    - Use cases do not need to be known in advance.
- **Data Handling**:
    - Stores raw data directly from source systems.
    - Data is transformed later based on analytical needs.
- **Governance Consideration**:
    - Data is classified, protected, and governed.
    - Not a dumping ground for unmanaged data.
- **Architecture Nature**:
    - Reference architecture.
    - Independent of specific technologies.

#### ‚úîÔ∏è 09 - Data Lake Technologies and Deployment
- **Deployment Options**:
    - Cloud object storage.
        - Example:
            - Amazon S3
    - Distributed systems.
        - Example:
            - Apache Hadoop
- **Storage Platforms**:
    - Relational databases.
    - NoSQL repositories.
- **Supporting Capabilities**:
    - Agile data exploration.
    - Designed for analysts and data scientists.

#### ‚úîÔ∏è 10 - Benefits of Data Lakes
- **Data Variety Support**:
    - Unstructured data.
        - Documents
        - Emails
        - PDFs
    - Semi-structured data.
        - JSON
        - XML
        - CSV
        - Logs
    - Structured relational data
- **Scalability**:
    - Easily scales from terabytes to petabytes.
- **Schema Flexibility**:
    - Eliminates upfront schema and transformation design.
- **Data Reusability**:
    - Enables multiple use cases from the same raw data.
    - Supports unknown future analytical needs.
- **Data Lake Vendors**:
    - Amazon
    - Cloudera
    - Google
    - IBM
    - Informatica
    - Microsoft
    - Oracle
    - SAS
    - Snowflake
    - Teradata
    - Zaloni

### üóÇÔ∏è Video - Viewpoints: Data Lakehouses Explained

### üóÇÔ∏è Video - Viewpoints: Considerations for Choice of Data Repository

### üóÇÔ∏è Video - ETL, ELT, and Data Pipelines

#### ‚úîÔ∏è 01 - Overview of Data Movement Tools and Processes
- **ETL (Extract, Transform, Load)**: A process used to move and prepare data from source systems to destination systems for analysis.
    - Converts raw data into analysis-ready data.
- **ELT (Extract, Load, Transform)**: A variation of ETL where data is transformed after being loaded into the target system.
- **Data Pipelines**: A broader concept that covers the entire journey of moving data from one system to another.
    - ETL and ELT can be considered subsets of data pipelines.

#### ‚úîÔ∏è 02 - ETL: Extract, Transform, and Load Process
- **ETL**: An automated process for gathering, preparing, and storing data for reporting and analytics.
    - Collects raw data from identified sources.
    - Cleans, standardizes, and transforms data.
    - Loads data into a data repository.

#### ‚úîÔ∏è 03 - Extract Phase in ETL
- **Extract**: The step where data is collected from source systems.
    - **Batch Processing**:
        - Data is moved in large chunks at scheduled intervals.
        - Tools include Stitch and Blendo.
    - **Stream Processing**:
        - Data is pulled and processed in real time.
        - Tools include Apache Samza, Apache Storm, and Apache Kafka.

#### ‚úîÔ∏è 04 - Transform Phase in ETL
- **Transform**: The application of rules and functions to convert raw data into usable data.
    - Standardizing date formats and units of measurement.
    - Removing duplicate data.
    - Filtering unnecessary data.
    - Enriching data.
        - Example: Splitting full names into first, middle, and last names.
    - Establishing key relationships across tables.
    - Applying business rules and data validations.

#### ‚úîÔ∏è 05 - Load Phase in ETL
- **Load**: The step where processed data is moved into a destination system or data repository.
    - **Initial Loading**:
        - Populating all data in the repository.
    - **Incremental Loading**:
        - Applying periodic updates and modifications.
    - **Full Refresh**:
        - Erasing existing data and reloading fresh data.
    - **Load Verification**:
        - Checking for missing or null values.
        - Monitoring server performance.
        - Detecting and recovering from load failures.

#### ‚úîÔ∏è 06 - ETL Usage and Tools
- **ETL Workloads**: Historically used for large-scale batch workloads.
    - Increasingly used for real-time streaming data with modern tools.
- **ETL Tools**:
    - IBM Infosphere Information Server
    - AWS Glue
    - Improvado
    - Skyvia
    - HEVO
    - Informatica PowerCenter

#### ‚úîÔ∏è 07 - ELT: Extract, Load, and Transform Process
- **ELT**: A process where data is transformed after being loaded into the target system.
    - Data is loaded first, then transformed within the destination system.
    - Commonly used with data lakes.
- **Destination Systems**:
    - Data lakes (most common).
    - Data warehouses.

#### ‚úîÔ∏è 08 - Advantages and Use Cases of ELT
- **ELT Use Cases**:
    - Processing large volumes of unstructured and non-relational data.
    - Ideal for big data environments.
- **Advantages**:
    - Shorter cycle between extraction and delivery.
    - Immediate ingestion of raw data as it becomes available.
    - Greater flexibility for analysts and data scientists.
    - Transformations applied only when needed for specific analyses.
        - Enables reuse of raw data for multiple use cases.
    - Avoids restructuring the entire warehouse for new use cases.

#### ‚úîÔ∏è 09 - Data Pipelines
- **Data Pipeline**: A high-performing system that moves data from source to destination.
    - Can support batch processing.
    - Can support streaming data.
    - Can combine batch and streaming approaches.
- **Streaming Data Pipelines**:
    - Data is processed continuously.
    - Useful for constantly updating data.
        - Example: Sensor data monitoring traffic.
- **Destinations**:
    - Typically a data lake.
    - Can also include applications or visualization tools.

#### ‚úîÔ∏è 10 - Data Pipeline Tools
- **Data Pipeline Solutions**:
    - Apache Beam
    - Airflow
    - DataFlow

### üóÇÔ∏è Video - Data Integration Platforms

#### ‚úîÔ∏è 01 - Definition of Data Integration
- **Data Integration**: A discipline comprising practices, architectural techniques, and tools that ingest, transform, combine, and provision data across various data types.
    - Defined by Gartner as an organization-wide capability.
    - Supports data movement and unification across heterogeneous systems.

#### ‚úîÔ∏è 02 - Usage Scenarios of Data Integration
- **Usage Scenarios**:
    - Data consistency across applications
    - Master data management
    - Data sharing between enterprises
    - Data migration and consolidation

#### ‚úîÔ∏è 03 - Data Integration in Analytics and Data Science
- **Analytics and Data Science Context**: Data integration enables analytics-focused data delivery.
    - Accessing, queueing, or extracting data from operational systems
    - Transforming and merging extracted data
        - Can be done logically or physically
    - Data quality and governance
    - Delivering integrated data for analytics purposes

#### ‚úîÔ∏è 04 - Example: Customer Data Integration
- **Customer Data Integration**: Making customer data available for analytics.
    - Extracting customer data from operational systems
        - Sales systems
        - Marketing systems
        - Finance systems
    - Providing a unified view of combined data
        - Enables users to access, query, and manipulate data
            - Used for statistics, analytics, and visualizations

#### ‚úîÔ∏è 05 - Relationship Between Data Integration, ETL, and Data Pipelines
- **Data Integration vs Data Pipeline**:
    - Data integration focuses on combining disparate data into a unified view.
    - Data pipelines cover the entire journey of data from source to destination.
- **ETL**: A process within data integration.
    - Used as part of a data pipeline to perform integration tasks.

#### ‚úîÔ∏è 06 - Core Capabilities of Modern Data Integration Solutions
- **Pre-built Connectors and Adapters**:
    - Databases
    - Flat files
    - Social media data
    - APIs
    - CRM and ERP applications
- **Open-source Architecture**:
    - Provides flexibility
    - Avoids vendor lock-in
- **Processing Optimization**:
    - Batch processing for large-scale data
    - Continuous data streaming
- **Big Data Integration**:
    - Increasingly influences platform selection

#### ‚úîÔ∏è 07 - Additional Functional Requirements
- **Data Quality and Governance**:
    - Ensures accuracy and reliability
- **Compliance and Security**:
    - Meets regulatory and organizational requirements
- **Portability**:
    - Supports deployment across environments
        - Single cloud
        - Multi-cloud
        - Hybrid cloud

#### ‚úîÔ∏è 08 - Commercial Data Integration Platforms
- **IBM Data Integration Tools**:
    - Information Server for IBM
    - Cloud Pak for Data
    - IBM Cloud Pak for Integration
    - IBM Data Replication
    - IBM Data Virtualization Manager
    - IBM InfoSphere Information Server on Cloud
    - IBM InfoSphere DataStage
- **Talend Data Integration Tools**:
    - Talend Data Fabric
    - Talend Cloud
    - Talend Data Catalog
    - Talend Data Management
    - Talend Big Data
    - Talend Data Services
    - Talend Open Studio
- **Other Vendors**:
    - SAP
    - Oracle
    - Denodo
    - SAS
    - Microsoft
    - Qlik
    - TIBCO

#### ‚úîÔ∏è 09 - Open-source Frameworks and iPaaS
- **Open-source Frameworks**:
    - Dell Boomi
    - Jitterbit
    - SnapLogic
- **Integration Platform as a Service (iPaaS)**:
    - Cloud-based, hosted integration services
    - Delivered via virtual private cloud or hybrid cloud
    - Examples:
        - Adeptia Integration Suite
        - Google Cloud Cooperation 534
        - IBM Application Integration Suite on Cloud
        - Informatica Integration Cloud

#### ‚úîÔ∏è 10 - Evolution of the Data Integration Space
- **Data Integration Evolution**:
    - Driven by growth in data volume and variety
    - Influenced by new technologies
    - Increasing importance in business decision-making

### üóÇÔ∏è Video - Viewpoints: Tools, Databases, and Data Repositories of Choice

## ‚≠êÔ∏è L3 - Big Data Platforms

### üóÇÔ∏è Video - Foundations of Big Data

#### ‚úîÔ∏è 01 - The Emergence of Big Data
- **Big Data**: Vast amounts of data generated by people, tools, and machines through digital interactions.
    - Originates from daily activities such as travel, workouts, and entertainment.
- **Big Data Definition (Ernst and Young)**: Dynamic, large, and disparate volumes of data requiring innovative and scalable technologies.
    - Used to derive real-time business insights related to consumers, risk, profit, and performance.

#### ‚úîÔ∏è 02 - Core Characteristics of Big Data
- **The V‚Äôs of Big Data**:
    - Velocity
    - Volume
    - Variety
    - Veracity
    - Value

#### ‚úîÔ∏è 03 - Velocity
- **Velocity**: The speed at which data is generated and accumulated.
    - Data generation is continuous and never stops.
    - Near real-time and real-time streaming technologies enable fast processing.
        - Supported by local and cloud-based technologies
    - Example:
        - Hours of video footage uploaded to YouTube every 60 seconds

#### ‚úîÔ∏è 04 - Volume
- **Volume**: The scale and amount of data being stored.
    - Driven by:
        - Increase in data sources
        - Higher resolution sensors
        - Scalable infrastructure
    - Example:
        - Approximately 2.5 quintillion bytes of data generated daily
            - Equivalent to about 10 million Blu-ray DVDs

#### ‚úîÔ∏è 05 - Variety
- **Variety**: The diversity of data types and sources.
    - **Structured Data**:
        - Organized into rows and columns
        - Stored in relational databases
    - **Unstructured Data**:
        - No predefined structure
        - Includes tweets, blog posts, images, videos, and numbers
    - Data Sources:
        - Machines
        - People
        - Processes
            - Internal and external to organizations
    - Drivers:
        - Mobile technologies
        - Social media
        - Wearable technologies
        - Geo technologies
        - Video
    - Example:
        - Text, pictures, films, sound, health data, and IoT device data

#### ‚úîÔ∏è 06 - Veracity
- **Veracity**: The quality, accuracy, and trustworthiness of data.
    - Attributes:
        - Consistency
        - Completeness
        - Integrity
        - Ambiguity
    - Drivers:
        - Cost considerations
        - Need for traceability
    - Challenge:
        - Determining whether data is real or false in the digital era
    - Example:
        - 80% of data is unstructured and requires categorization, analysis, and visualization

#### ‚úîÔ∏è 07 - Value
- **Value**: The ability to turn data into meaningful benefits.
    - Not limited to profit
    - Can include:
        - Medical benefits
        - Social benefits
        - Customer satisfaction
        - Employee or personal satisfaction
    - Main motivation for investing in Big Data understanding

#### ‚úîÔ∏è 08 - Big Data Analytics Challenges and Tools
- **Scalability Challenge**: Traditional data analysis tools are not feasible for massive data sets.
- **Distributed Computing Tools**:
    - Apache Spark
    - Hadoop and its ecosystem
    - Enable:
        - Extracting
        - Loading
        - Analyzing
        - Processing data across distributed resources
- **Outcome**:
    - New insights and knowledge
    - Enhanced customer connections
    - Improved and enriched services

#### ‚úîÔ∏è 09 - Data Journey in the Digital World
- **Data Journey**: Personal data travels through global systems for analysis and returns as insights.
    - Generated by:
        - Smartwatches
        - Smartphones
        - Fitness trackers
    - Processed through:
        - Big Data analytics
    - Result:
        - Improved user experiences and services

### üóÇÔ∏è Video - Big Data Processing Tools: Hadoop, HDFS, Hive, and Spark

#### ‚úîÔ∏è 01 - Big Data Processing Technologies Overview
- **Big Data Processing Technologies**: Technologies that enable working with large volumes of structured, semi-structured, and unstructured data.
    - Used to derive value from Big Data.
- **Open-source Technologies**: Core tools discussed for big data analytics.
    - Apache Hadoop
    - Apache Hive
    - Apache Spark

#### ‚úîÔ∏è 02 - Apache Hadoop: Purpose and Architecture
- **Apache Hadoop**: A Java-based open-source framework for distributed storage and processing of large datasets.
    - Operates across clusters of computers.
- **Node**: A single computer in a Hadoop system.
- **Cluster**: A collection of nodes working together.
- **Scalability**:
    - Scales from a single node to many nodes.
    - Each node provides local storage and computation.

#### ‚úîÔ∏è 03 - Hadoop Capabilities and Use Cases
- **Hadoop Capabilities**:
    - Distributed storage and processing
    - Reliable, scalable, and cost-effective storage
    - No strict data format requirements
- **Supported Data Types**:
    - Structured data
    - Semi-structured data
    - Unstructured data
        - Streaming audio
        - Video
        - Social media sentiment
        - Clickstream data
- **Enterprise Use Cases**:
    - Real-time, self-service access for stakeholders
    - Cost optimization of enterprise data warehouses
        - Moving ‚Äúcold‚Äù data to Hadoop-based systems

#### ‚úîÔ∏è 04 - Hadoop Distributed File System (HDFS)
- **HDFS (Hadoop Distributed File System)**: A distributed storage system for big data.
    - Runs on multiple commodity hardware nodes connected by a network
- **Core Characteristics**:
    - File partitioning across nodes
    - Parallel access to data
    - Fault tolerance through replication

#### ‚úîÔ∏è 05 - HDFS Data Storage and Fault Tolerance
- **File Partitioning**:
    - Large files are split into smaller blocks.
    - Blocks are distributed across multiple nodes.
- **Replication**:
    - Each block is replicated on multiple nodes by default.
    - Prevents data loss when a node fails
- **Example**:
    - A phone directory split alphabetically across different servers

#### ‚úîÔ∏è 06 - Benefits of HDFS
- **Parallel Processing**:
    - Computation runs on nodes where data is stored.
- **Data Locality**: Moving computation closer to data.
    - Reduces network congestion
    - Increases throughput
- **Additional Benefits**:
    - Fast recovery from hardware failures
    - High-throughput access to streaming data
    - Support for very large datasets
        - Hundreds of nodes per cluster
    - Portability across hardware platforms and operating systems

#### ‚úîÔ∏è 07 - Apache Hive: Data Warehousing on Hadoop
- **Apache Hive**: An open-source data warehouse software built on Hadoop.
    - Used for querying and analyzing large datasets
- **Storage Layer**:
    - HDFS
    - Other systems such as Apache HBase
- **Query Interface**:
    - SQL-based access to data

#### ‚úîÔ∏è 08 - Hive Use Cases and Limitations
- **Best Suited For**:
    - ETL
    - Reporting
    - Data analysis
- **Limitations**:
    - High query latency
        - Due to Hadoop‚Äôs sequential scan design
    - Read-based system
        - Not suitable for transaction processing
        - Poor fit for high write workloads

#### ‚úîÔ∏è 09 - Apache Spark: Real-time Data Processing
- **Apache Spark**: A general-purpose distributed data processing engine.
    - Designed for complex analytics and real-time processing
- **Key Use Cases**:
    - Interactive analytics
    - Stream processing
    - Machine learning
    - Data integration
    - ETL

#### ‚úîÔ∏è 10 - Spark Architecture and Capabilities
- **In-memory Processing**:
    - Data processed in memory for high speed
    - Spills to disk only when memory is constrained
- **Language Interfaces**:
    - Java
    - Scala
    - Python
    - R
    - SQL
- **Deployment Options**:
    - Standalone cluster
    - On top of Hadoop
- **Data Source Compatibility**:
    - HDFS
    - Hive
- **Key Advantage**:
    - Fast processing of streaming data and real-time analytics

### üóÇÔ∏è Video - Viewpoints: Impact of Big Data on Data Engineering





